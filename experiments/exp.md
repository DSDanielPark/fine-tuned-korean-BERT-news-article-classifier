
## Experiments
### [`mxnet.gluon`](https://mxnet.apache.org/versions/1.7/api/python/docs/api/gluon/index.html)

The Mode 1, Mode 2, and Mode 3 experiments were designed to see how well a subject could classify 8 topics into the body and headlines of a news article.
  - `Mode 1`: It learns data containing only the body of news articles as input.
  - `Mode 2`: It learns data containing only news article titles as input.
  - `Mode 3`: It learns data containing both news article titles and body as input.

<br>

Mode 4 and Mode 5 are designed to create and experiment with a kind of distilled BERT model.
- This experiment is designed *to see if it can be fine-tuned a bit faster based on the clustering information generated by SBERT.*
- In this experiment, the information clustered by SBERT, a multilingual model, is used to fine-tune the KO-BERT model.
  - `Mode 4`: Cluster information generated from SBERT, a multilingual model, is used as model input, and this experiment used the mxnet framework in the same way as Modes 1 to 3.
  - `Mode 5`: Clustering information generated from SBERT was input to the initial layer using the PyTorch framework. The results of this experiment may be slightly different from Modes 1 to 4, so they are not described.
    - However, as in Mode 4, it was confirmed through several iterations that almost similar accuracy can be reached faster if you have cluster information generated from SBERT.


- You can see whole experiments result in [`experments/exp.md`](https://github.com/DSDanielPark/news-article-classification-using-KO-BERT/blob/main/experiments/exp.md) 
- In the case of EXP2 and EXP9, it was repeatedly performed to track and observe the learning rate, confirming similar learning patterns.

| No | Condition | Best Test Accuracy | Epoch | 
|:---:|:---|:---:|:---:|
EXP1 | mode=1, batch_size=32 | 0.8895 | at Epoch 12
EXP2 | mode=1, batch_size=32, epoch=20 without early stopping. | 0.8895 | at epoch 12
EXP3 | mode=1, batch_size=16 | 0.8800 | at epoch 4
EXP4 | mode=1, batch_size=32 repeat of exp1 for check effect of randomness. | 0.8864 | at epoch 7
EXP5 | mode=1, batch_size=32 | 0.8874 | at epoch 7 
EXP6 | mode=2, batch_size=32 | 0.8269 | at epoch 7
EXP7 | mode=3, batch_size=32 | 0.8864 | at epoch 6
EXP8 | mode=4, batch_size=32, cluster_numb=8 | 0.8789 | at epoch 3
EXP9 | mode=4, batch_size=32, cluster_numb=16 | 0.8895 | at epoch 5
EXP10 | mode=4, batch_size=32, cluster_numb=32 | 0.8641 | at epoch 2
EXP11 | mode=4, batch_size=32, cluster_numb=64 | 0.8885 | at epoch 7

<br>




### Result Values
- Early stopping condition setted very tightly. Because I 
- If you want to use scheduler and earlystopper, check ['MXNet Gluon Fit API document'](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/training/fit_api_tutorial.html). and example ['Gluon callback function example notebook'](https://ts.gluon.ai/dev/tutorials/advanced_topics/trainer_callbacks.html)
- MXNet Glucon customized CallBack function을 사용하셔도 되지만, 그것보단 토치 문법으로 클래스로 변형 및 구현해서 mxnet NDArray에서 numpy.array로 변경하시는 것을 추천드립니다.  MXNet Glucon Implementation의 경우, MXNet 독자적인 Object Type을 사용하므로, numpy나 익숙하신 토치 문법으로 library로 수정 변형하시기 조금 더 번거롭습니다.
- MANet Framwork의 경우, 제대로 된 유지 보수가 진행되고 있지 않음을 확인하였으므로, 파이토치 프레임워크 사용을 추천드립니다. 특히, MXNet의 GPU 사용과 가속화와 관련된 이슈는 정상적으로 해결되지 않은 상태로 이슈가 오픈되어 있는 것을 확인하였으며, 환경 설정을 다시 수행해야하실 수도 있습니다. 


<br>

`exp1`: mode=1, batch_size=32

```
[Epoch 1 Batch 50/69] loss=23.0777, lr=0.0000352941, acc=0.334
Test Acc : 0.8099787685774947

[Epoch 2 Batch 50/69] loss=9.6362, lr=0.0000461039, acc=0.796
Test Acc : 0.8301486199575372

[Epoch 3 Batch 50/69] loss=6.7212, lr=0.0000405844, acc=0.869
Test Acc : 0.8535031847133758

[Epoch 4 Batch 50/69] loss=5.2704, lr=0.0000350649, acc=0.889
Test Acc : 0.8428874734607219

[Epoch 5 Batch 50/69] loss=4.0792, lr=0.0000295455, acc=0.918
Test Acc : 0.8588110403397028

[Epoch 6 Batch 50/69] loss=3.0294, lr=0.0000240260, acc=0.937
Test Acc : 0.8609341825902336

[Epoch 7 Batch 50/69] loss=2.2892, lr=0.0000185065, acc=0.956
Test Acc : 0.8641188959660298

[Epoch 8 Batch 50/69] loss=1.7187, lr=0.0000129870, acc=0.970
Test Acc : 0.8779193205944799

[Epoch 9 Batch 50/69] loss=1.2948, lr=0.0000074675, acc=0.979
Test Acc : 0.8832271762208068

[Epoch 10 Batch 50/69] loss=1.1464, lr=0.0000019481, acc=0.981
Test Acc : 0.8842887473460722
```

`exp2`: mode=1, epoch=20, batch_size=32

```
[Epoch 1 Batch 50/69] loss=22.7232, lr=0.0000176471, acc=0.343
Test Acc : 0.7664543524416136

[Epoch 2 Batch 50/69] loss=9.7546, lr=0.0000426471, acc=0.811
Test Acc : 0.8630573248407644

[Epoch 3 Batch 50/69] loss=5.5016, lr=0.0000480583, acc=0.873
Test Acc : 0.8842887473460722

[Epoch 4 Batch 50/69] loss=4.1820, lr=0.0000453074, acc=0.911
Test Acc : 0.8726114649681529

[Epoch 5 Batch 50/69] loss=3.0402, lr=0.0000425566, acc=0.936
Test Acc : 0.856687898089172

[Epoch 6 Batch 50/69] loss=2.4332, lr=0.0000398058, acc=0.949
Test Acc : 0.881104033970276

[Epoch 7 Batch 50/69] loss=1.6609, lr=0.0000370550, acc=0.964
Test Acc : 0.8726114649681529

[Epoch 8 Batch 50/69] loss=1.2232, lr=0.0000343042, acc=0.977
Test Acc : 0.8853503184713376

[Epoch 9 Batch 50/69] loss=0.8617, lr=0.0000315534, acc=0.984
Test Acc : 0.8757961783439491

[Epoch 10 Batch 50/69] loss=0.6065, lr=0.0000288026, acc=0.989
Test Acc : 0.8630573248407644

[Epoch 11 Batch 50/69] loss=0.5606, lr=0.0000260518, acc=0.988
Test Acc : 0.8821656050955414

[Epoch 12 Batch 50/69] loss=0.3497, lr=0.0000233010, acc=0.994
Test Acc : 0.8895966029723992

[Epoch 13 Batch 50/69] loss=0.2176, lr=0.0000205502, acc=0.997
Test Acc : 0.8789808917197452

[Epoch 14 Batch 50/69] loss=0.1351, lr=0.0000177994, acc=0.999
Test Acc : 0.8768577494692145

[Epoch 15 Batch 50/69] loss=0.0565, lr=0.0000150485, acc=0.999
Test Acc : 0.8768577494692145

[Epoch 16 Batch 50/69] loss=0.0395, lr=0.0000122977, acc=0.999
Test Acc : 0.8789808917197452

[Epoch 17 Batch 50/69] loss=0.0303, lr=0.0000095469, acc=1.000
Test Acc : 0.8789808917197452

[Epoch 18 Batch 50/69] loss=0.0264, lr=0.0000067961, acc=1.000
Test Acc : 0.8768577494692145

[Epoch 19 Batch 50/69] loss=0.0256, lr=0.0000040453, acc=1.000
Test Acc : 0.8779193205944799

[Epoch 20 Batch 50/69] loss=0.0249, lr=0.0000012945, acc=1.000
Test Acc : 0.8779193205944799
```

`exp3`: mode=1, batch_size=16

```
[Epoch 1 Batch 50/138] loss=24.3469, lr=0.0000176471, acc=0.231
[Epoch 1 Batch 100/138] loss=15.3135, lr=0.0000352941, acc=0.463
Test Acc : 0.8205944798301487

[Epoch 2 Batch 50/138] loss=8.7927, lr=0.0000480583, acc=0.801
[Epoch 2 Batch 100/138] loss=7.6007, lr=0.0000461165, acc=0.814
Test Acc : 0.856687898089172

[Epoch 3 Batch 50/138] loss=6.0856, lr=0.0000425566, acc=0.866
[Epoch 3 Batch 100/138] loss=4.7736, lr=0.0000406149, acc=0.878
Test Acc : 0.861995753715499

[Epoch 4 Batch 50/138] loss=4.4498, lr=0.0000370550, acc=0.916
[Epoch 4 Batch 100/138] loss=4.0218, lr=0.0000351133, acc=0.916
Test Acc : 0.8800424628450106
```

`exp4`: mode=1, batch_size=32
- repeat of exp1 for check learning tendency randomness.

```

[Epoch 1 Batch 50/69] loss=22.0188, lr=0.0000352941, acc=0.388
Test Acc : 0.8301486199575372

[Epoch 2 Batch 50/69] loss=8.2453, lr=0.0000461039, acc=0.820
Test Acc : 0.8184713375796179

[Epoch 3 Batch 50/69] loss=5.7065, lr=0.0000405844, acc=0.876
Test Acc : 0.8312101910828026

[Epoch 4 Batch 50/69] loss=4.9642, lr=0.0000350649, acc=0.902
Test Acc : 0.8715498938428875

[Epoch 5 Batch 50/69] loss=3.2247, lr=0.0000295455, acc=0.936
Test Acc : 0.8779193205944799

[Epoch 6 Batch 50/69] loss=2.8431, lr=0.0000240260, acc=0.941
Test Acc : 0.8757961783439491

[Epoch 7 Batch 50/69] loss=2.0676, lr=0.0000185065, acc=0.960
Test Acc : 0.886411889596603

```


- `exp5`: mode=1, batch_size=32
```
[Epoch 1 Batch 50/69] loss=23.3761, lr=0.0000142857, acc=0.311
Validation loss decreased (inf --> 1.182866).  
Test Acc : EvalMetric: {'accuracy': 0.7282377919320594}
1
100%
[Epoch 2 Batch 50/69] loss=10.8605, lr=0.0000345238, acc=0.791
Validation loss decreased (1.182866 --> 0.542432).  
Test Acc : EvalMetric: {'accuracy': 0.8439490445859873}

[Epoch 3 Batch 50/69] loss=7.2317, lr=0.0000494819, acc=0.838
Validation loss decreased (0.542432 --> 0.500014).  
Test Acc : EvalMetric: {'accuracy': 0.8641188959660298}

[Epoch 4 Batch 50/69] loss=4.3731, lr=0.0000472798, acc=0.907
Test Acc : EvalMetric: {'accuracy': 0.8598726114649682}

[Epoch 5 Batch 50/69] loss=3.2376, lr=0.0000450777, acc=0.924
Validation loss decreased (0.500014 --> 0.469267).  
Test Acc : EvalMetric: {'accuracy': 0.8853503184713376}

[Epoch 6 Batch 50/69] loss=2.2822, lr=0.0000428756, acc=0.951
Test Acc : EvalMetric: {'accuracy': 0.8662420382165605}

[Epoch 7 Batch 50/69] loss=1.5121, lr=0.0000406736, acc=0.972
Test Acc : EvalMetric: {'accuracy': 0.8874734607218684}
```

- `exp6`: mode=2, batch_size=32

```
[Epoch 1 Batch 50/69] loss=24.4246, lr=0.0000142857, acc=0.245
Validation loss decreased (inf --> 1.461474).  
Test Acc : EvalMetric: {'accuracy': 0.529723991507431}

[Epoch 2 Batch 50/69] loss=14.7511, lr=0.0000345238, acc=0.659
Validation loss decreased (1.461474 --> 0.969628).  
Test Acc : EvalMetric: {'accuracy': 0.7335456475583864}

[Epoch 3 Batch 50/69] loss=8.6145, lr=0.0000494819, acc=0.805
Validation loss decreased (0.969628 --> 0.741432). 
Test Acc : EvalMetric: {'accuracy': 0.7983014861995754}

[Epoch 4 Batch 50/69] loss=5.8329, lr=0.0000472798, acc=0.874
Validation loss decreased (0.741432 --> 0.713357). 
Test Acc : EvalMetric: {'accuracy': 0.8195329087048833}e

[Epoch 5 Batch 50/69] loss=4.4638, lr=0.0000450777, acc=0.902
Validation loss decreased (0.713357 --> 0.668929).  
Test Acc : EvalMetric: {'accuracy': 0.8152866242038217}

[Epoch 6 Batch 50/69] loss=3.9290, lr=0.0000428756, acc=0.921
Test Acc : EvalMetric: {'accuracy': 0.7983014861995754}

[Epoch 7 Batch 50/69] loss=2.4797, lr=0.0000406736, acc=0.949
Test Acc : EvalMetric: {'accuracy': 0.826963906581741}
```


- `exp7`: mode=3, batch_size=32

```
[Epoch 1 Batch 50/69] loss=24.4015, lr=0.0000142857, acc=0.251
Validation loss decreased (inf --> 1.242470).  Saving model ...
Test Acc : EvalMetric: {'accuracy': 0.7059447983014862}

[Epoch 2 Batch 50/69] loss=10.8851, lr=0.0000345238, acc=0.817
Validation loss decreased (1.242470 --> 0.536473).  
Test Acc : EvalMetric: {'accuracy': 0.8556263269639066}

[Epoch 3 Batch 50/69] loss=6.0983, lr=0.0000494819, acc=0.856
Test Acc : EvalMetric: {'accuracy': 0.8471337579617835}

[Epoch 4 Batch 50/69] loss=4.6641, lr=0.0000472798, acc=0.907
Test Acc : EvalMetric: {'accuracy': 0.8641188959660298}

[Epoch 5 Batch 50/69] loss=4.2402, lr=0.0000450777, acc=0.908
Validation loss decreased (0.536473 --> 0.498767).  
Test Acc : EvalMetric: {'accuracy': 0.8736730360934183}

[Epoch 6 Batch 50/69] loss=2.5094, lr=0.0000428756, acc=0.952
Validation loss decreased (0.498767 --> 0.498005).  
Test Acc : EvalMetric: {'accuracy': 0.886411889596603}

[Epoch 7 Batch 50/69] loss=1.7585, lr=0.0000406736, acc=0.964
Test Acc : EvalMetric: {'accuracy': 0.8556263269639066}

[Epoch 8 Batch 50/69] loss=1.5864, lr=0.0000384715, acc=0.969
Test Acc : EvalMetric: {'accuracy': 0.861995753715499}
```

- `exp8`: mode=4, batch_size=32, cluster_numb=8
```
[Epoch 1 Batch 50/69] loss=24.5230, lr=0.0000142857, acc=0.266
Validation loss decreased (inf --> 1.481091).  
Test Acc : EvalMetric: {'accuracy': 0.5010615711252654}

[Epoch 2 Batch 50/69] loss=13.1529, lr=0.0000345238, acc=0.731
Validation loss decreased (1.481091 --> 0.498874).  
Test Acc : EvalMetric: {'accuracy': 0.8673036093418259}

[Epoch 3 Batch 50/69] loss=5.7367, lr=0.0000494819, acc=0.871
Validation loss decreased (0.498874 --> 0.473925).  
Test Acc : EvalMetric: {'accuracy': 0.8789808917197452}

[Epoch 4 Batch 50/69] loss=4.3689, lr=0.0000472798, acc=0.901
Test Acc : EvalMetric: {'accuracy': 0.8726114649681529}

[Epoch 5 Batch 50/69] loss=3.2106, lr=0.0000450777, acc=0.934
Test Acc : EvalMetric: {'accuracy': 0.8503184713375797}
```

- `exp9`: mode=4, batch_size=32,  cluster_numb=16

```
[Epoch 1 Batch 50/69] loss=23.3296, lr=0.0000142857, acc=0.335
Validation loss decreased (inf --> 1.196755). 
Test Acc : EvalMetric: {'accuracy': 0.6942675159235668}

[Epoch 2 Batch 50/69] loss=10.6445, lr=0.0000345238, acc=0.812
Validation loss decreased (1.196755 --> 0.585845).  
Test Acc : EvalMetric: {'accuracy': 0.8312101910828026}

[Epoch 3 Batch 50/69] loss=6.1032, lr=0.0000494819, acc=0.863
Validation loss decreased (0.585845 --> 0.505442).  
Test Acc : EvalMetric: {'accuracy': 0.8545647558386412}

[Epoch 4 Batch 50/69] loss=4.3970, lr=0.0000472798, acc=0.902
Validation loss decreased (0.505442 --> 0.473047).  
Test Acc : EvalMetric: {'accuracy': 0.8694267515923567}

[Epoch 5 Batch 50/69] loss=3.2859, lr=0.0000450777, acc=0.928
Validation loss decreased (0.473047 --> 0.464250).  
Test Acc : EvalMetric: {'accuracy': 0.8895966029723992}
```


- `exp10`: mode=4, batch_size=32, cluster_numb=32

```
[Epoch 1 Batch 50/69] loss=23.2733, lr=0.0000142857, acc=0.284
Validation loss decreased (inf --> 1.147490).  
Test Acc : EvalMetric: {'accuracy': 0.7462845010615711}

[Epoch 2 Batch 50/69] loss=11.1283, lr=0.0000345238, acc=0.795
Validation loss decreased (1.147490 --> 0.510178).  
Test Acc : EvalMetric: {'accuracy': 0.8641188959660298}

[Epoch 3 Batch 50/69] loss=5.8019, lr=0.0000494819, acc=0.870
Test Acc : EvalMetric: {'accuracy': 0.8503184713375797}

[Epoch 4 Batch 50/69] loss=5.1935, lr=0.0000472798, acc=0.878
Test Acc : EvalMetric: {'accuracy': 0.8439490445859873}
```

- `exp11`: mode=4, batch_size=32, cluster_numb=64

```
[Epoch 1 Batch 50/69] loss=24.5099, lr=0.0000142857, acc=0.282
Validation loss decreased (inf --> 1.365160).  
Test Acc : EvalMetric: {'accuracy': 0.6390658174097664}

[Epoch 2 Batch 50/69] loss=12.2433, lr=0.0000345238, acc=0.766
Validation loss decreased (1.365160 --> 0.536511).  
Test Acc : EvalMetric: {'accuracy': 0.8588110403397028}

[Epoch 3 Batch 50/69] loss=5.9422, lr=0.0000494819, acc=0.866
Test Acc : EvalMetric: {'accuracy': 0.8174097664543525}

[Epoch 4 Batch 50/69] loss=4.2666, lr=0.0000472798, acc=0.901
Test Acc : EvalMetric: {'accuracy': 0.8460721868365181}

[Epoch 5 Batch 50/69] loss=3.6225, lr=0.0000450777, acc=0.922
Validation loss decreased (0.536511 --> 0.505307).  
Test Acc : EvalMetric: {'accuracy': 0.8757961783439491}

[Epoch 6 Batch 50/69] loss=2.4287, lr=0.0000428756, acc=0.951
Test Acc : EvalMetric: {'accuracy': 0.8651804670912951}

[Epoch 7 Batch 50/69] loss=1.5741, lr=0.0000406736, acc=0.970
Validation loss decreased (0.505307 --> 0.491683).  
Test Acc : EvalMetric: {'accuracy': 0.8885350318471338}

[Epoch 8 Batch 50/69] loss=1.5167, lr=0.0000384715, acc=0.972
Test Acc : EvalMetric: {'accuracy': 0.8598726114649682}

[Epoch 9 Batch 50/69] loss=1.2343, lr=0.0000362694, acc=0.978
Test Acc : EvalMetric: {'accuracy': 0.8662420382165605}
```






## Result
| |MODE`1`|MODE`2`|MODE`3`|MODE`4`|MODE`5`|
|:---:|:---:|:---:|:---:|:---:|:---:|
|Data|article body only|articel title only|article with title|article with title which have clustered infomation from SBERT model|article with title|
|Model|KO-BERT|KO-BERT|KO-BERT|KO-BERT, SBERT|KO-BERT, SBERT with clustered infomation in initial hidden layer|
|TestSet Accuracy|0.8895|0.8269|0.8864|0.8895|-|
|Remark|Model architecture and all conditions in models were fixed.|-|-|-|Model architecture and initial layer are changed only in `MODE5`.|
- As I mentioned above 'Experiments' section, All conditions in whole pipeline was fixed except initial layer in MODE5 and Pre-trained Models were KO-BERT [3], SBERT [5].
<br>

<br><br>




### [PyTorch](https://pytorch.org/hub/huggingface_pytorch-transformers/)

```
epoch 1 batch id 1 loss 2.0699574947357178 train acc 0.140625
epoch 1 train acc 0.6144642857142858
epoch 1 test acc 0.8515398550724638

epoch 2 batch id 1 loss 0.7810292840003967 train acc 0.90625
epoch 2 train acc 0.8980357142857143
epoch 2 test acc 0.8694293478260869

epoch 3 batch id 1 loss 0.32714197039604187 train acc 0.953125
epoch 3 train acc 0.9234821428571428
epoch 3 test acc 0.8602807971014492

## Train parameters
max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 10
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5
```
